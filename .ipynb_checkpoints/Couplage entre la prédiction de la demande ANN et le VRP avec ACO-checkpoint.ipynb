{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d06b46ef-4774-42be-9edf-f714e8a9a1ac",
   "metadata": {},
   "source": [
    "L'énoncé du problème du Vehicle Routing Problem (VRP) avec une flotte de véhicules N80 K10 et la \n",
    "prédiction de la demande client par réseaux de neurones (ANN) est complexe. Voici un plan détaillé pour \n",
    "résoudre ce problème en utilisant l'algorithme de colonies de fourmis (ACO) avec une interface utilisateur \n",
    "interactive pour les paramètres. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb1ad7a-2fd5-4f2f-87b1-b10f3ef0c732",
   "metadata": {},
   "source": [
    "# 1. Prédictions de la demande client par ANN "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944b6925-79ba-44a2-bda4-91e338df0465",
   "metadata": {},
   "source": [
    "### Entrainement de data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6b84201-7d93-4912-8dc7-65ce3949f330",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import os \n",
    "import sweetviz as sv\n",
    "import missingno as msno\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input , Dropout\n",
    "\n",
    "from matplotlib import image as mpimg\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64d35176-be65-4584-9644-b18354bb28df",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'VRP/data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#import historical Product Data \u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVRP/data.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1881\u001b[0m     f,\n\u001b[0;32m   1882\u001b[0m     mode,\n\u001b[0;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1889\u001b[0m )\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'VRP/data.csv'"
     ]
    }
   ],
   "source": [
    "#import historical Product Data \n",
    "df = pd.read_csv(r'VRP/data.csv')  # Adjust the path based on your actual structure STR404"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12c8437-967c-4df7-a6fe-2f9261ab3c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3690f95b-7ec0-4a7a-9167-da0a2813c4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the report\n",
    "report = sv.analyze(df)\n",
    "# To display the report in a Jupyter Notebook:\n",
    "report.show_notebook()\n",
    "# To save the report as an HTML file:\n",
    "report.show_html(r'VRP/OverviewSTR404.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bccda8-c2ba-4e59-9d6c-7cbcc72ce53b",
   "metadata": {},
   "source": [
    "based on the previous we can conclude that there's no missing data \n",
    "for sake of simplicity we will delete warehouses as if we are having one warehouse and we will take into the account \n",
    "not only two years but from 2012 to 2016 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d100f2b6-158a-4409-81f3-71b1f32cc03d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#look for missing data\n",
    "msno.matrix(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a34cada-a4af-484c-9a57-5e5a1538d812",
   "metadata": {},
   "source": [
    "As we can see there's not much missing except for some on the part of date "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35011f5-a5da-44f5-a88b-02f2fcedc3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to drop missing values (which we don't have on our case)\n",
    "df.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95288d14-8fec-4aab-95d2-977da531e433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete warehouse :\n",
    "df = df.drop(columns=['Warehouse'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fc906f-8d18-4c0f-ba17-0830a5f6bb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['Order_Demand'].dtype)  # Check the column type\n",
    "\n",
    "df['Order_Demand'] = pd.to_numeric(df['Order_Demand'], errors='coerce')  # Converts to int/float\n",
    "\n",
    "order = df['Order_Demand'].to_numpy()\n",
    "\n",
    "# Using Python built-in functions\n",
    "min_order = min(order)\n",
    "max_order = max(order)\n",
    "\n",
    "print(\"Minimum Order Amount:\", min_order)\n",
    "print(\"Maximum Order Amount:\", max_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f153495-3829-4a42-b748-9bef23815846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's relabel some of them                                                                      STR404\n",
    "# Initialize label encoders\n",
    "product_encoder = LabelEncoder()\n",
    "category_encoder = LabelEncoder()\n",
    "\n",
    "# Apply encoding\n",
    "df['Product_Code'] = product_encoder.fit_transform(df['Product_Code'])\n",
    "\n",
    "df['Product_Category'] = category_encoder.fit_transform(df['Product_Category'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e35aca-4575-4589-8196-9e9be4ac747a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_order_demand_over_time(df): \n",
    "    df['Date'] = pd.to_datetime(df['Date'], errors='coerce')  # Convert date column\n",
    "    date_demand = df.groupby(\"Date\")[\"Order_Demand\"].sum()  # Aggregate by date\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.plot(date_demand.index, date_demand.values, marker='o', linestyle='-', color='red', alpha=0.7)\n",
    "    plt.title(\"Order Demand Over Time\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Total Order Demand\")\n",
    "    plt.grid(True)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "    #STR404\n",
    "\n",
    "# Call the function\n",
    "plot_order_demand_over_time(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08cfad9-d9b5-4928-9051-bd665a4d0d8f",
   "metadata": {},
   "source": [
    "The order demand fluctuates significantly over time.\n",
    "There is a huge increase in demand after 2012, meaning either:\n",
    "Business growth or product popularity increased.\n",
    "Data collection became more consistent.\n",
    "New products or clients entered the market.\n",
    "The high variation suggests seasonal trends or occasional spikes in demand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f35029-b942-4bd0-9552-37b2e6e71d2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_order_demand_per_product(df):\n",
    "    product_demand = df.groupby(\"Product_Code\")[\"Order_Demand\"].sum().sort_values()\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    product_demand.plot(kind='bar', color='blue', alpha=0.7)\n",
    "    plt.title(\"Total Order Demand per Product Code\")\n",
    "    plt.xlabel(\"Product Code\")\n",
    "    plt.ylabel(\"Total Order Demand\")\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Call the function\n",
    "plot_order_demand_per_product(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304647e4-de77-4ca3-9ec0-dbeca43f3607",
   "metadata": {},
   "source": [
    "only a few product codes have extreme demand.\n",
    "Many products have low or negligible demand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b470b9aa-be8d-4b5c-a81d-384b603a8a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_order_demand_per_category(df):\n",
    "    category_demand = df.groupby(\"Product_Category\")[\"Order_Demand\"].sum().sort_values()\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    category_demand.plot(kind='bar', color='green', alpha=0.7)\n",
    "    plt.title(\"Total Order Demand per Product Category\")\n",
    "    plt.xlabel(\"Product Category\")\n",
    "    plt.ylabel(\"Total Order Demand\")\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.show()\n",
    "\n",
    "# Call the function\n",
    "plot_order_demand_per_category(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982965b9-9124-4a9b-bad6-a28ace463ab0",
   "metadata": {},
   "source": [
    "Some product categories have way higher demand than others.\n",
    "**Category_019** dominates, meaning:\n",
    "It is the **best-selling** or most frequently ordered category.\n",
    "Other categories contribute far less to total demand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a1bb91-ee39-4926-ab76-b46c20c80e5f",
   "metadata": {},
   "source": [
    "## Normalizing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cf23db-aace-4253-b18a-5d2d1b66804d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "df['Order_Demand'] = scaler.fit_transform(df[['Order_Demand']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3697f1-8a81-42b9-82de-6ca51224da0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_order_demand_over_time(df):\n",
    "    df['Date'] = pd.to_datetime(df['Date'], errors='coerce')  # Convert date column\n",
    "    date_demand = df.groupby(\"Date\")[\"Order_Demand\"].sum()  # Aggregate by date\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.plot(date_demand.index, date_demand.values, marker='o', linestyle='-', color='red', alpha=0.7)\n",
    "    plt.title(\"Order Demand Over Time --Normalized\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Total Order Demand\")\n",
    "    plt.grid(True)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "\n",
    "# Call the function\n",
    "plot_order_demand_over_time(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c03111a-1301-44aa-b33d-050f956a3bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_order_demand_per_category(df):\n",
    "    category_demand = df.groupby(\"Product_Category\")[\"Order_Demand\"].sum().sort_values()\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    category_demand.plot(kind='bar', color='green', alpha=0.7)\n",
    "    plt.title(\"Total Order Demand per Product Category--Normalized\")\n",
    "    plt.xlabel(\"Product Category\")\n",
    "    plt.ylabel(\"Total Order Demand\")\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.show()\n",
    "\n",
    "# Call the function\n",
    "plot_order_demand_per_category(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0414cb-928f-458c-b4df-beef33f1426a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_order_demand_per_product(df):\n",
    "    product_demand = df.groupby(\"Product_Code\")[\"Order_Demand\"].sum().sort_values()\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    product_demand.plot(kind='bar', color='blue', alpha=0.7)\n",
    "    plt.title(\"Total Order Demand per Product Code--Normalized\")\n",
    "    plt.xlabel(\"Product Code\")\n",
    "    plt.ylabel(\"Total Order Demand\")\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Call the function\n",
    "plot_order_demand_per_product(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b23c54-693e-431a-9ba3-404dbf8806e6",
   "metadata": {},
   "source": [
    "### Now we can tell they are further normalized\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81dbd190-d937-416d-91dd-79813dca704e",
   "metadata": {},
   "source": [
    "# Building the ANN to predict how much demand there's on certain dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440cfcc1-39d0-47bf-9e49-8b62386227e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting Data\n",
    "df_daily = df.groupby(\"Date\", as_index=False)[\"Order_Demand\"].sum()\n",
    "df_daily = df_daily.sort_values(by=\"Date\")\n",
    "\n",
    "# Sort data by date (to maintain time sequence)\n",
    "df_daily=df_daily.sort_values(by=\"Date\")\n",
    "df_daily['Order_Demand'] = np.log1p(df_daily['Order_Demand'])  # log(1 + x) prevents log(0) issues\n",
    "\n",
    "\n",
    "# Define training period (e.g., last 3 years for training)\n",
    "split_date = \"2015-06-01\"  # Adjust based on dataset range\n",
    "# Split into train and test sets based on date\n",
    "train_data = df_daily[df_daily[\"Date\"] < split_date]\n",
    "test_data = df_daily[df_daily[\"Date\"] >= split_date]\n",
    "\n",
    "print(f\"Training data size: {train_data.shape}\")\n",
    "print(f\"Testing data size: {test_data.shape}\")\n",
    "\n",
    "\n",
    "train_data.head(1213)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2dcb9c-f00e-4bea-b324-84ed90d8f4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_time_series_features(data, target_col='Order_Demand', lags=30):\n",
    "    \"\"\"\n",
    "    Create lag features for time series forecasting\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : pandas.DataFrame\n",
    "        Input data containing the target column\n",
    "    target_col : str\n",
    "        Name of the target column to create lags from\n",
    "    lags : int\n",
    "        Number of lag features to create\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        DataFrame with added lag features\n",
    "    \"\"\"\n",
    "    df = data.copy()\n",
    "    for lag in range(1, lags + 1):\n",
    "        df[f'lag_{lag}'] = df[target_col].shift(lag)\n",
    "    return df\n",
    "\n",
    "# Create features\n",
    "train_data = create_time_series_features(train_data)\n",
    "test_data = create_time_series_features(test_data)\n",
    "\n",
    "# Remove NaN values\n",
    "train_data = train_data.dropna()\n",
    "test_data = test_data.dropna()\n",
    "\n",
    "# Separate features and target\n",
    "feature_columns = [col for col in train_data.columns if col.startswith('lag_')]\n",
    "X_train = train_data[feature_columns]\n",
    "y_train = train_data['Order_Demand']\n",
    "X_test = test_data[feature_columns]\n",
    "y_test = test_data['Order_Demand']\n",
    "\n",
    "# Convert to numeric types\n",
    "X_train = X_train.astype(float)\n",
    "y_train = y_train.astype(float)\n",
    "X_test = X_test.astype(float)\n",
    "y_test = y_test.astype(float)\n",
    "#STR404\n",
    "# Convert to numpy arrays if needed\n",
    "X_train = X_train.to_numpy()\n",
    "y_train = y_train.to_numpy()\n",
    "X_test = X_test.to_numpy()\n",
    "y_test = y_test.to_numpy()\n",
    "\n",
    "print(f\"Training feature shape: {X_train.shape}\")\n",
    "print(f\"Testing feature shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b726641-2ce1-4bd7-b44c-f70bb8787ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "plt.title(\"Activations Image\")\n",
    "plt.xlabel(\"X pixel scaling\")\n",
    "plt.ylabel(\"Y pixels scaling\")\n",
    " \n",
    "image = mpimg.imread(r'VRP/Activation.png')\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d16d1e-a5f3-4867-91cf-ae12ec2ae188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train ANN\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "\n",
    "model = Sequential([\n",
    "\n",
    "    Input(shape=(X_train.shape[1],)),\n",
    "    Dense(512, activation='relu'),  \n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.3),  # Prevent overfitting\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1)  \n",
    "\n",
    "])\n",
    "# Define learning rate schedule\n",
    "lr_schedule = ExponentialDecay(\n",
    "    initial_learning_rate=0.001,  # Start with a learning rate of 0.001\n",
    "    decay_steps=1000,  # Apply decay every 1000 steps\n",
    "    decay_rate=0.95,  # Reduce learning rate by 5% each step\n",
    "    staircase=True  # Apply stepwise reduction\n",
    ")\n",
    "\n",
    "# Apply to Adam optimizer\n",
    "optimizer = Adam(learning_rate=lr_schedule)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=optimizer, loss=\"mse\", metrics=['mae'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7513c6f-f6b5-4613-9419-4bb41643183f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the model\n",
    "# Train model\n",
    "y_train = np.log1p(y_train)\n",
    "y_test = np.log1p(y_test)\n",
    "print(type(X_train))  # Should be <class 'pandas.DataFrame.STR404'>\n",
    " # Should be numerical (float or int)\n",
    "\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=1000, batch_size=32, validation_data=(X_test, y_test), verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9acd915-0dd9-4a34-8939-9e22b1df177f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "mse = model.evaluate(X_test, y_test)\n",
    "print(f\"Test MSE: {mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff6e443-f3fc-427c-911b-4cdbe84d3ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract MSE and MAE from the evaluation results\n",
    "mse, mae = mse  # Unpack the list values STR404\n",
    "\n",
    "# Convert MSE and MAE back to original scale\n",
    "original_mse = scaler.inverse_transform([[mse]])[0, 0]\n",
    "original_mae = scaler.inverse_transform([[mae]])[0, 0]\n",
    "\n",
    "# Print results\n",
    "print(f\"Test MSE STR404 in original scale: {original_mse}\")\n",
    "print(f\"Test MAE in original scale: {original_mae}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2160f42-bf97-4582-b99f-90ad63e0f4ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2139a30-7450-4259-80ff-2564b628923c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_original = scaler.inverse_transform(y_pred)\n",
    "y_test_original = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(y_test_original, label=\"Actual Demand\", color=\"blue\")\n",
    "plt.plot(y_pred_original, label=\"Predicted Demand\", color=\"red\", linestyle=\"dashed\")\n",
    "plt.legend()\n",
    "plt.title(\"Actual vs. Predicted Order Demand\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e38f5ba-8ac7-4114-bb40-2f45facc52bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def predict_next_30_days(model, X_test, scaler):\n",
    "    \"\"\"\n",
    "    Predicts the next 30 days of demand using the trained ANN model.\n",
    "    \"\"\"\n",
    "    # Start with the last available test data row STR404\n",
    "    last_30_days = X_test[-1].reshape(1, -1)  # Get last test input row\n",
    "    future_predictions = []\n",
    "\n",
    "    for _ in range(30):  # Predict for the next 30 days\n",
    "        next_day_pred = model.predict(last_30_days)[0, 0]  # Predict next demand\n",
    "        future_predictions.append(next_day_pred)  # Store the prediction\n",
    "\n",
    "        # Shift input data (remove oldest, add newest prediction)\n",
    "        last_30_days = np.roll(last_30_days, -1)  # Shift left\n",
    "        last_30_days[0, -1] = next_day_pred  # Insert new prediction\n",
    "\n",
    "    # Convert predictions back to the original scale\n",
    "    future_predictions_original = scaler.inverse_transform(np.array(future_predictions).reshape(-1, 1))\n",
    "\n",
    "    return future_predictions_original\n",
    "\n",
    "# Run the prediction function\n",
    "future_demand = predict_next_30_days(model, X_test, scaler)\n",
    "\n",
    "# Plot the future predictions\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(future_demand, label=\"Predicted Demand (Next 30 Days)\", color=\"red\", linestyle=\"dashed\")\n",
    "plt.xlabel(\"Days Ahead\")\n",
    "plt.ylabel(\"Predicted Order Demand\")\n",
    "plt.title(\"Predicted Order Demand for the Next 30 Days\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Print predicted values\n",
    "print(\"Predicted Order Demand for Next 30 Days:\\n\", future_demand.flatten())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a159415-f0b0-4050-9422-1666c5ee71c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# les 80 clients:\n",
    "# # Création du graphe\n",
    "G = nx.Graph()\n",
    "\n",
    "# Ajout des nœuds (chaque client est un nœud)\n",
    "for i in range(num_clients):\n",
    "    G.add_node(i, pos=(np.random.rand() * 100, np.random.rand() * 100))  # Position aléatoire des clients\n",
    "\n",
    "# Ajout des arêtes (liaisons entre clients avec poids = distance)\n",
    "for i in range(num_clients):\n",
    "    for j in range(i + 1, num_clients):  # Éviter les doublons\n",
    "        G.add_edge(i, j, weight=distance_matrix[i, j])\n",
    "\n",
    "# Récupérer les positions des nœuds\n",
    "pos = nx.get_node_attributes(G, 'pos')\n",
    "\n",
    "# Dessiner le graphe\n",
    "plt.figure(figsize=(12, 8))\n",
    "nx.draw(G, pos, with_labels=True, node_color=\"lightblue\", node_size=300, font_size=8)\n",
    "\n",
    "# Ajouter les poids (distances) sur les arêtes\n",
    "edge_labels = {(i, j): f\"{distance_matrix[i, j]} km\" for i, j in G.edges()}\n",
    "nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=6)\n",
    "\n",
    "plt.title(\"Graphe des Distances entre les Clients\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4610344-060d-4092-9985-8126af710a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# STEP 1: Predict demand using ANN (assuming ANN is already trained)\n",
    "# ----------------------------------------\n",
    "# Assume we have ANN demand predictions for each client for the next 30 days\n",
    "predicted_demand = np.random.randint(100, 500, size=(80,))  # Simulated ANN output\n",
    "\n",
    "# Normalize demand for ACO\n",
    "normalized_demand = predicted_demand / max(predicted_demand)  # Scale between 0-1\n",
    "\n",
    "\n",
    "# STEP 2: Generate Distance Matrix (VRP Input)\n",
    "# ----------------------------------------\n",
    "num_clients = 80\n",
    "num_vehicles = 10\n",
    "vehicle_capacity = 1000  # Max capacity of each vehicle\n",
    "max_time = 30  # Max allowed delivery time\n",
    "\n",
    "np.random.seed(42)\n",
    "distance_matrix = np.random.randint(1, 100, (num_clients, num_clients))\n",
    "np.fill_diagonal(distance_matrix, 0)  # No distance to self\n",
    "\n",
    "\n",
    "# STEP 3: Modify ACO to Prioritize Demand-Based Routes\n",
    "# ----------------------------------------\n",
    "num_ants = 20\n",
    "num_iterations = 100\n",
    "evaporation_rate = 0.1\n",
    "alpha = 1\n",
    "beta = 2\n",
    "Q = 100\n",
    "\n",
    "pheromones = np.ones((num_clients, num_clients)) * 0.1  # Initialize pheromones\n",
    "\n",
    "def aco_vrp_with_ann():\n",
    "    global pheromones\n",
    "    best_route = None\n",
    "    best_cost = float('inf')\n",
    "\n",
    "    for iteration in range(num_iterations):\n",
    "        routes = []\n",
    "        costs = []\n",
    "\n",
    "        for ant in range(num_ants):\n",
    "            route = [0]  # Start at the depot (client 0)\n",
    "            visited = set(route)\n",
    "\n",
    "            while len(visited) < num_clients:\n",
    "                current = route[-1]\n",
    "\n",
    "                # Modify probability using ANN demand predictions\n",
    "                probabilities = (pheromones[current] ** alpha) * ((1 / distance_matrix[current]) ** beta)\n",
    "                probabilities *= (normalized_demand + 0.1)  # Boost probability based on demand\n",
    "\n",
    "                probabilities[list(visited)] = 0  # Avoid revisiting\n",
    "                probabilities /= probabilities.sum()  # Normalize\n",
    "\n",
    "                next_client = np.random.choice(range(num_clients), p=probabilities)\n",
    "                route.append(next_client)\n",
    "                visited.add(next_client)\n",
    "\n",
    "            routes.append(route)\n",
    "            total_distance = sum(distance_matrix[route[i], route[i+1]] for i in range(len(route)-1))\n",
    "            costs.append(total_distance)\n",
    "\n",
    "        best_index = np.argmin(costs)\n",
    "        if costs[best_index] < best_cost:\n",
    "            best_route = routes[best_index]\n",
    "            best_cost = costs[best_index]\n",
    "\n",
    "        pheromones *= (1 - evaporation_rate)  # Evaporate pheromones\n",
    "        for i in range(len(best_route)-1):\n",
    "            pheromones[best_route[i], best_route[i+1]] += Q / best_cost  # Reinforce best path\n",
    "\n",
    "    return best_route, best_cost\n",
    "\n",
    "# Run ACO with ANN predictions\n",
    "optimal_route, optimal_cost = aco_vrp_with_ann()\n",
    "\n",
    "# Display results\n",
    "print(\"Optimized Route Using ANN Demand Predictions:\", optimal_route)\n",
    "print(\"Total Cost of Route:\", optimal_cost)\n",
    "\n",
    "# Visualize results\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(range(len(predicted_demand)), predicted_demand, color='red', alpha=0.7)\n",
    "plt.xlabel(\"Client ID\")\n",
    "plt.ylabel(\"Predicted Demand\")\n",
    "plt.title(\"Predicted Demand from ANN for VRP\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0851e7a3-0b52-4200-8978-2fbce9ae6099",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Simulated client positions (randomly placed on a map)\n",
    "num_clients = 80\n",
    "np.random.seed(42)\n",
    "client_positions = {i: (np.random.rand() * 100, np.random.rand() * 100) for i in range(num_clients)}\n",
    "\n",
    "# Example of an optimized route found by ACO\n",
    "optimized_route = optimal_route\n",
    "\n",
    "# Create a graph\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# Add nodes (clients)\n",
    "for client, pos in client_positions.items():\n",
    "    G.add_node(client, pos=pos)\n",
    "\n",
    "# Add edges (connections based on the optimized route)\n",
    "edges = [(optimized_route[i], optimized_route[i+1]) for i in range(len(optimized_route)-1)]\n",
    "G.add_edges_from(edges)\n",
    "\n",
    "# Get positions for visualization\n",
    "pos = nx.get_node_attributes(G, 'pos')\n",
    "\n",
    "# Draw the graph\n",
    "plt.figure(figsize=(12, 8))\n",
    "nx.draw(G, pos, with_labels=True, node_color=\"lightblue\", edge_color=\"red\", width=2, node_size=500, font_size=8)\n",
    "\n",
    "# Highlight depot (starting point)\n",
    "nx.draw_networkx_nodes(G, pos, nodelist=[0], node_color=\"green\", node_size=800, label=\"Depot\")\n",
    "\n",
    "plt.title(\"Optimized Vehicle Routing Plan (ACO + ANN Demand)\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58bea84e-5c81-491b-93ef-682b80c00783",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Save the trained ANN model\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mann_demand_prediction.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Save the data scaler (if used for normalization)\u001b[39;00m\n\u001b[0;32m      8\u001b[0m joblib\u001b[38;5;241m.\u001b[39mdump(scaler, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscaler.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "import joblib  # or pickle\n",
    "import tensorflow as tf\n",
    "\n",
    "# Save the trained ANN model\n",
    "model.save(\"ann_demand_prediction.h5\")\n",
    "\n",
    "# Save the data scaler (if used for normalization)\n",
    "joblib.dump(scaler, \"scaler.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0926c1b-2a48-41be-9103-0c6f779d7d10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
